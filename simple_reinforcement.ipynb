{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My attempt to replicate this:\n",
    "\n",
    "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0\n",
    "\n",
    "As interested in the creation of the environment as I am building and training the agent, so going to build both. \n",
    "\n",
    "S - start<br>\n",
    "O - floor<br>\n",
    "D - doom<br>\n",
    "G - goal\n",
    "\n",
    "## Preview\n",
    "\n",
    "<p align=\"center\"><img src=\"./q-learning-gif.gif\"/></p>\n",
    "\n",
    "---\n",
    "\n",
    "###### Update 2018-01-18\n",
    "\n",
    "It appears to work. I had to do a bit of a hack to get the Q-table working properly. I'd go back to fix it but I have a bunch of other projects in the queue, so moving on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "from IPython.display import clear_output # for the hacky render piece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create environment\n",
    "# S - start\n",
    "# O - floor\n",
    "# D - doom\n",
    "# G - goal\n",
    "\n",
    "# note: not going to do the random piece that throws off the agent\n",
    "# like openai does. could, but not going to at the moment\n",
    "\n",
    "test_env = np.array([\n",
    "\n",
    "    ['S', 'O', 'D'],\n",
    "    ['O', 'D', 'O'],\n",
    "    ['O', 'D', 'O'],\n",
    "    ['O', 'O', 'O'],\n",
    "    ['D', 'G', 'D']\n",
    "   \n",
    "])\n",
    "\n",
    "actions = {\n",
    "           \"up\":1,\n",
    "           \"right\":2,\n",
    "           \"down\":3,\n",
    "           \"left\":4\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the main func that will be doing the acting\n",
    "\n",
    "def actor(action,\n",
    "          state,\n",
    "          env = test_env):\n",
    "\n",
    "    if action == 1:\n",
    "        \n",
    "        update = state[0]-1\n",
    "        \n",
    "        if update < 0:\n",
    "            pass\n",
    "        else:\n",
    "            state[0] = update\n",
    "\n",
    "    elif action == 2: \n",
    "        \n",
    "        update = state[1]+1\n",
    "        \n",
    "        if update > 2:\n",
    "            pass\n",
    "        else:\n",
    "            state[1] = update\n",
    "\n",
    "    elif action == 3:\n",
    "        \n",
    "        update = state[0]+1\n",
    "       \n",
    "        if update < 0:\n",
    "            pass\n",
    "        else:\n",
    "            state[0] = update\n",
    "\n",
    "    else:\n",
    "        \n",
    "        update = state[1]-1\n",
    "\n",
    "        if update < 0:\n",
    "            pass\n",
    "        else:\n",
    "            state[1] = update\n",
    "            \n",
    "    # return vals        \n",
    "\n",
    "    i0 = state[0]\n",
    "    i1 = state[1]\n",
    "\n",
    "    updated_state = env[i0][i1]\n",
    "    \n",
    "    if updated_state == \"G\":\n",
    "        \n",
    "        DONE = True\n",
    "        reward = 1\n",
    "    \n",
    "    elif updated_state == \"D\":\n",
    "        \n",
    "        DONE = True\n",
    "        reward = 0\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        DONE = False\n",
    "        reward = 0\n",
    "        \n",
    "    return state,DONE,reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will show the actor moving the env for each episode\n",
    "\n",
    "def displayer(state,\n",
    "              n_episode,\n",
    "              env=test_env):\n",
    "    \n",
    "    copy_env = env.copy()\n",
    "    \n",
    "    copy_env[state[0]][state[1]] = \"*\" \n",
    "\n",
    "    env_string = '\\n'.join([''.join(['{:4}'.format(item) for item in row]) for row in copy_env])\n",
    "\n",
    "    sys.stdout.write(\"Episode \"+str(n_episode)+'\\n\\r'+env_string)\n",
    "    time.sleep(.05)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define execution vars\n",
    "\n",
    "start_state = [0,0]\n",
    "all_rewards = {} # key (n_episode) : value (array of vals to take the max of)\n",
    "\n",
    "n_episodes = 250\n",
    "\n",
    "# the below is ugly. I could avoid it, but I'd have to redo the way I create the env.\n",
    "# going with it for now, maybe will return (probs not)\n",
    "\n",
    "states_to_state = {\n",
    "    \n",
    "       '0':[0,0],\n",
    "       '1':[0,1],\n",
    "       '2':[0,2],\n",
    "       '3':[1,0],\n",
    "       '4':[1,1],\n",
    "       '5':[1,2],\n",
    "       '6':[2,0],\n",
    "       '7':[2,1],\n",
    "       '8':[2,2],\n",
    "       '9':[3,0],\n",
    "       '10':[3,1],\n",
    "       '11':[3,2],\n",
    "       '12':[4,0],\n",
    "       '13':[4,1],\n",
    "       '14':[4,2]\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "### the Q-learning/table vars\n",
    "\n",
    "# in essence: for each state we need to get the value of each action\n",
    "# as runs start to become more successful, then values start to become more of a signal\n",
    "\n",
    "#Initialize table with all zeros\n",
    "\n",
    "QTABLE = np.zeros([test_env.shape[0]*test_env.shape[1],len(actions)])\n",
    "\n",
    "lr = .8\n",
    "y = .95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(1,n_episodes):\n",
    "    \n",
    "    n_step = 0\n",
    "    running_states = [start_state]\n",
    "    \n",
    "    while n_step <99: # don't let agent go pass 99 steps per episode\n",
    "\n",
    "        step_rewards = [] # for collecting all rewards\n",
    "        \n",
    "        # convert raw state coordinates to corresponding Q-table number\n",
    "        sN = int(list(states_to_state.keys())[list(states_to_state.values()).index(running_states[n_step])])\n",
    "\n",
    "        a = np.argmax(QTABLE[sN,:] + np.random.randn(1,len(actions))*(1./(e+1))) # select action\n",
    "\n",
    "        ### main func\n",
    "        s,DONE,reward = actor(\n",
    "\n",
    "            action = a,\n",
    "            state = list(running_states[n_step])\n",
    "\n",
    "        )\n",
    "        \n",
    "        sN1 = int(list(states_to_state.keys())[list(states_to_state.values()).index(s)]) \n",
    "\n",
    "        # display grid\n",
    "        #displayer(state=s,n_episode=e)\n",
    "        \n",
    "        # update Q-table\n",
    "        QTABLE[sN,a] = QTABLE[sN,a] + lr*(reward + y*np.max(QTABLE[sN1,:]) - QTABLE[sN,a]) \n",
    "        \n",
    "        # update latest state\n",
    "        running_states.append(s)\n",
    "\n",
    "        # update latest reward\n",
    "        step_rewards.append(reward)\n",
    "\n",
    "        # find reward per episode (0 or 1)\n",
    "        all_rewards[e] = max(step_rewards)\n",
    "\n",
    "        # increment step\n",
    "        n_step += 1\n",
    "\n",
    "        if DONE:\n",
    "\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
